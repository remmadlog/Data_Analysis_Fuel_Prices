---
title: "German Fuel Price Analysis 2014-2023"
output: rmarkdown::github_document
---

# Overview for fuel prices for 2022

## Loading and transforming

Loading data for 2022

```{r}
agg_2022 <- read.csv("Datasets/German Retail Fuel Price Data 2014-2023/aggregated_years/aggregated2022.csv")
```

Loading library `tidyverse`

```{r warning=FALSE,message=FALSE}
library(tidyverse)
```

Transforming `fuel` columns into one column plus an additional lable column

```{r}
stacked_2022 <- agg_2022 %>%
  pivot_longer(diesel:e10, names_to = "fuel", values_to = "price")
```

## Generating an overview

Generate a short overview of fuel prices for 2022 by month

```{r warning=FALSE,message=FALSE}
ov_month <- stacked_2022 %>%
  select(month, fuel, price) %>%
  filter(price>1) %>%
  group_by(month,fuel) %>%
  summarise(average = mean(price),
            min = min(price),
            max = max(price),
            median = median(price)
  )
```

```{r}
print(ov_month)
```

Generate a short overview of fuel prices for 2022 by station

```{r warning=FALSE,message=FALSE}
ov_station <- stacked_2022 %>%
  select(station_uuid, fuel, price) %>%
  # try to drop false entries
  filter(price>1) %>%
  group_by(station_uuid, fuel) %>%
  # only consider if we have at least 6 month of data
  filter(n()>5) %>%
  summarise(average = mean(price),
            min = min(price),
            max = max(price),
            median = median(price)
  ) %>%
  arrange(fuel,average)

```

```{r}
head(ov_station)
```

Sorted output of cheapest stations

```{r}
# cheapest diesel stations:
ov_station %>%
  filter(fuel == "diesel") %>%
  head()

# cheapest e5 stations:
ov_station %>%
  filter(fuel == "e5") %>%
  head()

# cheapest e10 stations:
ov_station %>%
  filter(fuel == "e10") %>%
  head()
```

## Plotting

Import `ggplot2` for plotting

```{r}
library(ggplot2)
```

Generate a line-plot showing the average price per month

```{r}
ggplot(data=ov_month, aes(x=month, y=average, group = fuel, color=fuel)) +
  geom_line(stat="identity") +
  labs(title = "Average fuel price per month in 2022",
        y = "price in euro",
        x = "Month") +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12)) +
  geom_text(aes(label= round(average,2)),  hjust=1.5, size=3, col = "black") +
  coord_cartesian(ylim=c(1.5,2.2))
```

# General overview of fuel price from 2014 to 2023

## Loading and transforming

Since we have one file for each year, we need to merge them into one file

```{r}
# generating a list of all files
list_of_files <- list.files(path = "Datasets/German Retail Fuel Price Data 2014-2023/aggregated_years",
                            pattern = "\\.csv$",
                            full.names = TRUE)

# merging all file in the list
agg_14_to_23 <- list_of_files %>%
  map_dfr(read.csv, header=TRUE, fill=TRUE)
```

As before we are transforming `fuel` columns into one column plus an additional lable column

```{r}
stacked_14_23 <- agg_14_to_23 %>%
  pivot_longer(diesel:e10, names_to = "fuel", values_to = "price")
```

## Generating an overview

We generate an overview too

```{r warning=FALSE,message=FALSE}
ov_year_month <- stacked_14_23 %>%
  select(year, month, fuel, price) %>%
  filter(price>1) %>%
  group_by(year,month,fuel) %>%
  summarise(average = mean(price),
            min = min(price),
            max = max(price),
            median = median(price)
  ) %>%
  arrange(year,month)
```

```{r}
print(ov_year_month)
```

If we want to generate a plot the show the fuel prices over time we have to combine the `year` and `month` into `date`. Therefore, we load the `zoo` library

```{r warning=FALSE,message=FALSE}
library(zoo)
```

Now we combine `year` and `month` into `date`

```{r warning=FALSE,message=FALSE}
# combine year and month to a new date column
ov_year_month$date <- zoo::as.yearmon(paste(ov_year_month$year, ov_year_month$month), "%Y %m")
```

```{r}
head(ov_year_month)
```

## Plotting

We are now able to create a line plot the provides the fuel prices from 2014 to 2023

```{r}
ggplot(data=ov_year_month, aes(x=date, y=average, group = fuel, color=fuel)) +
  geom_line(stat="identity") +
  labs(title = "Average fule price from 2014 to 2023",
        y = "price in euro",
        x = "Year/Month") +
  coord_cartesian(ylim=c(1,2.2))
```

Additionally, we may consider a comparison of the average price of, e.g. **e10**, over the years:

```{r}
ov_year_month %>%
        filter(fuel == "e10") %>%
        ggplot(aes(x=month, y=average, group = year, color=as.factor(year))) +
        geom_line(stat="identity") +
        labs(title = "Average e10 price per month grouped by year",
            y = "average e10 price in euro",
            x = "month") +
        scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12)) +
        coord_cartesian(ylim=c(1,2.2))
```

We can get a better feeling for a monthly behaviour if we normalize the price of, **hear e10**, for each year:

```{r}
temp <- ov_year_month
temp$norm_price_av <- ave(temp$average, temp$year, FUN=function(x) (x - min(x))/(max(x) - min(x)))
```

Let us plot a comparison of the normed average price of **e10** over the years:

```{r}
temp %>%
        filter(fuel == "e10") %>%
        filter(year != "2014") %>% # remove partial year
        filter(year != "2023") %>% # remove partial year
        ggplot(aes(x=month, y=norm_price_av, group = year, color=as.factor(year))) +
        scale_color_manual(values=c("midnightblue", "darkgreen", "red", "gold", "cyan", "blue", "black", "brown")) +
        geom_line(stat="identity") +
        labs(title = "Average e10 price per month grouped by year",
            y = "[0,1] normalization of average fuel price",
            x = "month") +
        scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12)) +
        coord_cartesian(ylim=c(0,1))
```

In general, we notice a reduce price in **January** and **December**.

In the data for **2020** we notice a huge price drop from **February** to **April**. This marks the start of the first Corona lockdown.

In the data for **2022** we see a high increase in **February**. This is probably due to the Russian war on Ukraine. We also notice a drop from **June** to **August**. In that time the german government provided tax reduction on fuel.

# Average cheapest Month

Using `stacked_14_23` to get an average price per month per fuel

## Generating an Overview

```{r warning=FALSE,message=FALSE}
ov_14_23_month <- stacked_14_23 %>%
  select(month, fuel, price) %>%
  filter(price>1) %>%
  group_by(month,fuel) %>%
  summarise(average = mean(price),
            min = min(price),
            max = max(price),
            median = median(price)
  ) %>%
  arrange(month)
```

```{r}
head(ov_14_23_month)
```

## Plotting

```{r}
ggplot(data=ov_14_23_month, aes(x=month, y=average, group = fuel, color=fuel)) +
  geom_line(stat="identity") +
  labs(title = "Average fule price from 2014 to 2023",
        y = "price in euro",
        x = "Month") +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11,12)) +
  geom_text(aes(label= round(average,2)),  vjust=1.5, size=3, col = "black") +
  coord_cartesian(ylim=c(1.25,1.525))
```

From 2014 to 2023 the month with the lower average price are **January**, **August** and **December**. For **diesel** the **winter** and **summer** month are less expansive compared to **spring** and **fall**.

# Location Comparison

In this section we will consider fuel prices based on there location.

## Loading and Transforming

Loading data containing station information.

```{r}
stations <- read.csv("Datasets/German Retail Fuel Price Data 2014-2023/stations.csv")
```

### Cleaning City Names - Writing

Let us see how many entries there are per **city**.

```{r}
stations %>% count(city) %>% head(10)
```

We notice that some city names - are all **capital** - have a leading **spacec** - have a **space** at the end - are `gelöscht`, `mehr aktiv`, `please delete - bitte loeschen` - only have one station - contain an **odd** symbole

We have to do something about this. While we can and will ignore some, e.g. `alsdorf h�ngen` should be the same as `alsdorf-hoengen`, we will make some changes to others.

Making all city names **lowercase**

```{r}
stations$city <- tolower(stations$city)
```

To remove a **space** at the beginning or et the end we use `trimws`

```{r}
stations$city <- trimws(stations$city)
```

We will ignore the rest for now, and deal with them if they bother us later.

Lets se how many entries there are per **city** again:

```{r}
stations %>% count(city) %>% head(10)
```

### Merging Tables

We will join the table `stations` to the table of all fuel prices from 2014 to 2023, namely `stacked_14_23` Before that, we select only the columns we want from `stations`

```{r}
stations_select <- stations %>%
  select(uuid,name,brand,post_code,city)
```

for the **joining** we use `library(dplyr)`

```{r warning=FALSE,message=FALSE}
library(dplyr)
```

While joining, we have to account for the different `station ID` names in the tables

```{r}
full_fuel_table <- left_join(stacked_14_23, stations_select, by=c("station_uuid"= "uuid"))
```

### Cleaning City Names - NaN and Blanks

Let us have a look at the first and last few entries if we sort `full_fuel_table` by **city**:

```{r}
temp <- full_fuel_table %>%
  arrange(city)
```

First 10 entries:

```{r}
print(head(temp,10))
```

Last 10 entries:

```{r}
print(tail(temp,10))
```

We notice: - some `city names` are **blank** - - here we still have `post_code` - some `city names` are **NaN** - - here we have no entry in `stations` for the ID in `stacked_14_23`

Since we want to compare cities, we remove every entry that does not have an entry in `post_code`. Removing rows with `NaN`:

```{r}
full_fuel_table_cleaned <-  na.omit(full_fuel_table)
```

Let us have a look at the first and last few entries if we sort `full_fuel_table_cleaned` by **city**:

```{r}
temp <- full_fuel_table_cleaned %>%
  arrange(city)
```

First 10 entries:

```{r}
print(head(temp,10))
```

Last 10 entries:

```{r}
print(tail(temp,10))
```

We still have **blank** city names, but for this case we can consider the post code.

### Cleaning City Names - Post Code

Create an overview of all stations that have a blank city entry:

```{r}
temp <- stations %>%
  select(uuid,city,post_code) %>%
  filter(city=="")
print(temp)
```

We get - `47929`, which is the post code of **grefrath** - `97688`, which is the post code of **bad kissingen** - `44357`, which is the post code of **dortmund** - , wich we will remove

> It would be best to change this in `stations` at the beginning but this document is ment to show the process. Therefore, I wil will try to add this information now without just reapplying the above.

Adding missing information in `stations` first:

```{r}
stations$city[stations$post_code == 47929] <- "grefrath"
stations$city[stations$post_code == 97688] <- "bad kissingen"
stations$city[stations$post_code == 44357] <- "dortmund"
```

Adding missing information in `full_fuel_table_cleaned`:

```{r}
full_fuel_table_cleaned$city[full_fuel_table_cleaned$post_code == 47929] <- "grefrath"
full_fuel_table_cleaned$city[full_fuel_table_cleaned$post_code == 97688] <- "bad kissingen"
full_fuel_table_cleaned$city[full_fuel_table_cleaned$post_code == 44357] <- "dortmund"
```

Now all that is left for now is removing **blank** entries in `postal_code` as well as **00000** entries.

```{r}
full_fuel_table_cleaned <- full_fuel_table_cleaned %>%
        filter(post_code != "") %>%
        filter(post_code != "00000")
```

We also get rid of odd prices:

```{r}
full_fuel_table_cleaned <- full_fuel_table_cleaned %>%
      filter(price>0.7) %>%                                 # would be better to use statistical analysis here
      filter(price<5)                                       # would be better to use statistical analysis here
```

## Generate an Overview of Fuel Price by location

~~Since a **city** can have a multiple **post codes** but a **post code** is tied to exactly one **city**, we will group by `post_code` and later match them with the corresponding city.~~

**A post code can have multiple place names and vice versa. This makes it quite hard to identify.**

See for example the post code **22113**. It references to a place in **Hamburg** as well as to **Oststeinbek**. The later one is not even part of the same region/state (Bundesland).

> This seems odd, since we added the city information above, but remember that this is ment as a learning experience. The goal is not getting a result fast, it is to overcome problems occurring on the way. So yes, we could have skipped the `city` cleaning part and just went with the `post_code` from the beginning.

To identify the post code and provide a city name, we use data set obtained [here][<https://www.datenbörse.net/item/Postleitzahlen-Datenbank_Deutschland>]. We had to replace the seperator `;` with `,`.

Loding the data:

```{r}
plz <- read.csv("Datasets/Postleitzahlen 2023 mit Bundesland.csv")
# column names: PLZ,ORT,ZUSATZ,BUNDESLAND 
# (post code, name of place (city name), additional information, region/state)
```

A joining attempted:

> `full_fuel_table_cleaned_plz <- left_join(full_fuel_table_cleaned, plz, by=c("post_code"= "PLZ"))`

resulted in a warning:

> Warning in left_join(full_fuel_table_cleaned, plz, by = c(post_code = "PLZ")) :
>
> Detected an unexpected many-to-many relationship between `x` and `y`.
>
> i Row 16480 of `x` matches multiple rows in `y`. \> i Row 6973 of `y` matches multiple rows in `x`.
>
> i If a many-to-many relationship is expected, set `relationship = >   "many-to-many"` to silence this warning.

Considering the file, on notice that if several places have the same post code, they are **not** combined in one place. Therefore, postal codes can occur multiple times. In order to fix that one could remove duplicates ,e.g. arbitrarily. We will not do so, but proceed by only focusing on the post code for now.

Create an overview of average fuel prices grouped by `post_code`:

```{r}
temp <- full_fuel_table_cleaned %>%
        select(year, month, fuel, price, brand, post_code) %>%
        group_by(post_code) %>%
        summarise(average = mean(price),
                  min = min(price),
                  max = max(price),
                  median = median(price),
                  size = n()              # group size, how many data entries were combined
  ) %>%
  arrange(average)

print(head(temp))
```

Short detour: We should think about `size`, since it is important to know how many entire were combined in order to evaluate if the quality of the outcome.

We have **10 years** wherein **2** are incomplete. For **2014** we have **7** month of data and for **2023** we have **4** month of data. Keeping that in mind, we end up with `8*12 + 7 + 4 = 107` different month.

Furthermore, we distinguish between **3** different fuels: `diesel, e5, e10`.

Combining this, a station that provides all three fuels over the full length of 107 month, should provide us with **321** data entries.

Let us count the entries we have per `station_uuid`:

```{r}
temp <- full_fuel_table_cleaned %>%
      group_by(station_uuid) %>%
      summarise(occurrence_of_uuid = n() ) %>%             # group size, how many data entries were combined
      arrange(occurrence_of_uuid)
```

Let us look at the first few entries

```{r}
print(head(temp,5))
```

Let us look at the last few entries

```{r}
print(tail(temp,5))
```

We notice that some stations do not provide us with enough information to use.

> We should think about how much information is enough. Reasons for missing data could be:
>
> -   closer over a time frame
>
> -   existence of the station is
>
> -   reduced collection of fuels
>
> There might be more reasons. Since 321 entries were perfect I think it would be fine if we go wth 50% of this as a threshold. One could provide more checks here, e.g. if the missing data is connected or grouped in some way (fuel, period of time), but I will not do that.

> In addition, we see that some station have more entries than they should have. Going back through all iterations I found the reason: It can happen that e.g. in aggregated2016.csv there is information about the 12th month of a station in aggregated2015.csv, but that information is also provided in aggregated2015.csv. Therefore, we sometimes get doubles of december entries. Since we usually consider the average of some group, it makes no difference for us at the moment. Therefore, I will not do anything about that right now, but if one wanted to something about this, one could just take the average.

In order to use the information we joint `temp` to `full_fuel_table_cleaned` by `uuid`:

```{r}
full_fuel_table_cleaned <- left_join(full_fuel_table_cleaned, temp, by="station_uuid")

```

Now we can get valuable results regarding fuel prices by post code.

```{r warning=FALSE,message=FALSE}
fuel_price_postcode <- full_fuel_table_cleaned %>%
        filter(occurrence_of_uuid>160) %>%    # roughly 50% all possible data per uuid
        group_by(post_code,year, month,fuel) %>%
        summarise(av_price = mean(price),
                  group_size = n())
```

Next we generate an overview for each year and each fuel:

```{r warning=FALSE,message=FALSE}
f <- "e10"  # Set FUEL
y <- 2022   # Set Year

temp <- fuel_price_postcode %>%
        filter(year == y) %>%
        filter(fuel == f) %>%
        arrange(av_price)
```

```{r}
print(head(temp,15))
```

## Map Plot

Next we identify the `post_code` with the corresponding state/region/Bundesland. We shall remember that this is not possible, since it happens that a post code occurs in two different sates, see, e.g. **22113**. Regardless we will do so. In cases as above we will count the post code for both states.

We start by joining `full_fuel_table_cleaned` with `plz`:

```{r warning=FALSE,message=FALSE}
fuel_price_state <- left_join(full_fuel_table_cleaned,plz,by = c("post_code" = "PLZ"))
```

We are presented with the following waring:

> Warning in left_join(full_fuel_table_cleaned, plz, by = c(post_code = "PLZ")) : Detected an unexpected many-to-many relationship between `x` and `y`. i Row 40 of `x` matches multiple rows in `y`. i Row 12091 of `y` matches multiple rows in `x`. i If a many-to-many relationship is expected, set `relationship =   "many-to-many"` to silence this warning.

We assumed that something like that might happen, since of the example **22113**. The problem here its, that this happens not because of a **state** but because of a **place**. In order to avoid that we will group `plz` by `BUNDESLAND` and `PLZ`.

```{r}
plz_state <- plz %>%
        group_by(BUNDESLAND,PLZ) %>%
        summarise(group_size = n())

print(head(plz_state),10)
```

Having a look at the outcome we notice - some entries of `PLZ` are not numerical - some entries of `BUNDESLAND` are blank, not a state or misspelled

Cleaning of `plz`:

```{r warning=FALSE,message=FALSE}
plz_clean <- plz %>%
        filter(BUNDESLAND != "")

plz_clean$BUNDESLAND <- trimws(plz_clean$BUNDESLAND)
plz_clean$BUNDESLAND[plz_clean$BUNDESLAND == "Anhalt"] <- "Sachsen-Anhalt"
plz_clean$BUNDESLAND[plz_clean$BUNDESLAND == "Baden"] <- "Baden-Württemberg"
plz_clean$BUNDESLAND[plz_clean$BUNDESLAND == "Bay"] <- "Bayern"
plz_clean$BUNDESLAND[plz_clean$BUNDESLAND == "Holst"] <- "Schleswig-Holstein"
plz_clean$BUNDESLAND[plz_clean$BUNDESLAND == "Holstein"] <- "Schleswig-Holstein"
plz_clean$BUNDESLAND[plz_clean$BUNDESLAND == "Mecklenburg"] <- "Mecklenburg-Vorpommern"


states_list <-  c("Baden-Wuerttemberg","Bayern","Berlin","Brandenburg","Bremen","Hamburg","Hessen","Mecklenburg-Vorpommern","Niedersachsen","Nordrhein-Westfalen","Rheinland-Pfalz","Saarland","Sachsen","Sachsen-Anhalt","Schleswig-Holstein","Thueringen")


plz_state <- plz_clean %>%
        filter(BUNDESLAND %in% states_list) %>%
        group_by(BUNDESLAND, PLZ) %>%
        summarise(group_size = n())
```

We now join again and group by `BUNDESLAND`, `year`, `month` and `fuel`

```{r warning=FALSE,message=FALSE}

temp <- left_join(full_fuel_table_cleaned,plz_state,by = c("post_code" = "PLZ"))
fuel_price_state <- temp%>%
        group_by(BUNDESLAND, year, month, fuel) %>%
        filter(BUNDESLAND %in% states_list) %>%
        summarise(av_price = median(price),
                  gr_size = n())
```

We now have a well-prepared data set, that can be used to get a overview of different fuel prices per state per year per month.

Loading some libraries:

```{r warning=FALSE,message=FALSE}
library(sf)
library(tidyverse)
theme_set(theme_bw())
library(giscoR)
```

To provide a geographical context we want to add the administrative boundaries of Germany on a federal-state scale.

```{r warning=FALSE,message=FALSE}
germany <- gisco_get_nuts(
  nuts_level = 1,
  resolution = 10,
  country = "Germany",
  year = 2021
)

# small changes due to our own data
germany$NUTS_NAME[germany$NUTS_NAME == "Thüringen"] <- "Thueringen"
germany$NUTS_NAME[germany$NUTS_NAME == "Baden-Württemberg"] <- "Baden-Wuerttemberg"
```

For the year **2022** and **e10** we then get

```{r}
temp <- fuel_price_state %>%
        filter(year == 2022) %>%
        filter(fuel == "e10") %>%
        select(BUNDESLAND, av_price) %>%
        group_by(BUNDESLAND) %>%
        summarise(av_price=mean((av_price)))

ger_end <- germany %>%
  left_join(temp, by = c("NUTS_NAME" = "BUNDESLAND"))

# plot(ger_end["av_price"])

# using ggplot2
ggplot(ger_end) +
  geom_sf(aes(fill = av_price))
```

We will now try to get an animated version. For this we use:

```{r warning=FALSE,message=FALSE}
library(gganimate)
```

We combine `month` and `year` to a `date` column:

```{r warning=FALSE,message=FALSE}
# combine year and month to a new date column
fuel_price_state$date <- zoo::as.yearmon(paste(fuel_price_state$year, fuel_price_state$month), "%Y %m")
```

Considering `e10:`

```{r}
temp <- fuel_price_state %>%
        filter(fuel == "e10") %>%
        select(BUNDESLAND, date,av_price) %>%
        group_by(BUNDESLAND, date) %>%
        summarise(av_price=mean((av_price)))

ger_end <- germany %>%
  left_join(temp, by = c("NUTS_NAME" = "BUNDESLAND"))


```

Plotting:

```{r}
p <- ggplot(ger_end) +
  geom_sf(aes(fill = av_price)) +
  labs( title = 'Year: {closest_state}') +
  scale_fill_gradient(low = '#709AE1', high = '#FD7446')+
  transition_states(date)
animate(p, fps=3)
```

Sadly we do not see so much differences in each time state. This make sense though, since fuel prices in Germany are primarily driven by supply and demand, influenced by global oil prices, and further affected by taxes and value-added tax.

For now, we will leve it at this.

# Prediction using ARIMA

## Preparation

Adding `date` column, using `year` and `month` columns.

```{r warning=FALSE,message=FALSE}
# combine year and month to a new date column
full_fuel_table_cleaned$date <- zoo::as.yearmon(paste(full_fuel_table_cleaned$year, full_fuel_table_cleaned$month), "%Y %m")
```

Creating a table to forcast **e10** prices:

```{r warning=FALSE,message=FALSE}
fc_e10 <- full_fuel_table_cleaned %>%
        filter(fuel == "e10") %>%
        group_by(date) %>%
        summarise(av_price=mean(price))
```

## Plotting the Time Series

Considering the price of **e10** over time.

```{r}
ggplot(fc_e10, aes(date, av_price)) + geom_line()
```

Creating a **logplot**:

```{r}
plot(diff(log(fc_e10$av_price)),type='l', main='log returns plot')
```

In order to perform reasonable modeling on our data, the time series must be stationary in the sense that the mean, the variance, and the covariance of the series should be constant with time. The **logplot** above seams stationary. We confirm this in the following blocks.

First we load the needed library.

```{r warning=FALSE,message=FALSE}
library(tseries)
```

We then test the stationary of the **log** data.

```{r}
adf.test(diff(log(fc_e10$av_price)), alternative="stationary", k=0)
```

## Determining Parameters

The Dickey-Fuller test returns a p-value of 0.01. The warning suggests tht this value is even smaller. This is resulting in the rejection of the null hypothesis and accepting the alternate, that the data is stationary.

ACF, or Auto-Correlation Function, measures the correlation between a time series and its past (lagged) values. Essentially, it shows how current values in the series are related to their previous values. This information is particularly useful for identifying the appropriate number (or order) of moving average (MA) terms to include in an ARIMA model.

```{r}
acf(diff(log(fc_e10$av_price)))
```

PACF, or Partial Auto-Correlation Function, measures the correlation between a time series and its lagged values after removing the effects of intermediate lags. Unlike ACF, which captures all correlations—including indirect ones—PACF isolates the direct relationship between a value and a specific lag. If a residual still contains useful information explainable by a particular lag, PACF will highlight it. This makes PACF especially valuable for determining the appropriate number (or order) of auto-regressive (AR) terms in an ARIMA model.

```{r}
pacf(diff(log(fc_e10$av_price)))
```

To determine the order of our parameters, we look at the difference in lags in the plots for the ACF and PACF. A significant drop after some lag in the plots, suggests that the ordered terms we should use for our parameters.

In oure case we see a drop in the ACF plot after 1 and in the PACF plot after 3. Resulting in an MR(1) and an AR(3).

## Building the Arima Modle

Loading the library first.

```{r warning=FALSE,message=FALSE}
library(forecast)
```

Our analysis above suggest an ARIMA with the parameters 3, 1, 1:

```{r}
(fit <- arima(fc_e10$av_price, c(3, 1, 1)))
```

Did we do good? In general a lower AIC score is better. To see if we could do better, we can just test for different parameters or just do a full parameter search.

For that we can use the `auto.arima` function

```{r}
fit_auto <- auto.arima(fc_e10$av_price, trace=TRUE)
```

We see that we can get slightly worse results using `ARIMA(0,0,3)` instead of `ARIMA(3,0,1)` This will be even worse, since `auto.arima` uses approximations in order to be faster. Refitting the modle gives us

```{r}
(fit <- arima(fc_e10$av_price, c(0, 1, 3)))
```

Therefor, we stick to initial `ARIMA(3,0,1)`.

```{r}
(fit <- arima(fc_e10$av_price, c(3, 1, 1)))
```

Let us see how the `ARIMA(3,0,1)` fits our data before doing the forcasting.

```{r}
plot(as.ts(fc_e10$av_price) )
lines(fitted(fit), col="red")
```

This looks quit pleasing, but we just tested the model on the training data.

### Forcasting

Now we continue with the forcasting for the next **6** month.

```{r}
futurVal <- forecast(fit,h=6)
plot(forecast(futurVal))
```

The dark blue area shows the 80% confidence interval, whereas the light blue shows the 95% confidence interval.
